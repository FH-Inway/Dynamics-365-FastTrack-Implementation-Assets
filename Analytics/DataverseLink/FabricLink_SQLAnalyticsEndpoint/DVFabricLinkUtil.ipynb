{"cells":[{"cell_type":"markdown","source":["# Notebook does following \n","\n","- üèóÔ∏è Step 1: Create case-insensitive Data Warehouse.\n","- üîπ Step 2: Connect to Synapse Serverless Database and get table list and schema information.\n","- üîÅ Step 3: Get parent and child tables information from local file.\n","- üõ†Ô∏è Step 4: Connect to Fabric Link lakehouse get table metadata and generate view ddl statements.\n","- üèÉ‚Äç‚ôÇÔ∏è Step 5: Connect to Fabric case in-senstive data warehouse and create views.\n","- üöÄ Step 6: Connect to Synaspe serverless virtual datawarehouse and collect views and dependencies.\n","- üöÄ Step 7: Connect to Fabric datawarehouse and deploy create views\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"97ea5c26-d6c2-4e71-a58c-217d10d63b4f"},{"cell_type":"code","source":["# --- Configuration Constants ---\n","WORKSPACE_ID        = \"6d5a10ea-5454-4b7d-bc59-9c620c3b6540\"\n","FABRIC_LH_DATABASE  = \"dataverse_analytics_lakehouse\"\n","FABRIC_WH_DATABASE  = \"dataverse_analytics_warehouse1\"\n","FABRIC_WH_SCHEMA    = \"edw\"\n","\n","DRIVER              = \"{ODBC Driver 18 for SQL Server}\"\n","SYNAPSE_SERVER      = \"d365analyticsfabricsynapse-ondemand.sql.azuresynapse.net\"\n","SYNAPSE_EDL_DATABASE= \"analytics.sandbox.operations.dynamics.com\"\n","SYNAPSE_EDL_SCHEMA  = \"dbo\"\n","SYNAPSE_EDL_CONST_COLUMN_NAME =\"_SysRowId\" \n","\n","SYNAPSE_DW_DATABASE = \"Dynamics365_DW\"\n","SYNAPSE_DW_SCHEMA   = \"dbo\"\n","SYNAPSE_DW_VIEWS    = \"CustomerDim\"\n","\n","GITHUB_RAW_BASE_URL = \"https://raw.githubusercontent.com/microsoft/Dynamics-365-FastTrack-Implementation-Assets/refs/heads/master/Analytics/DataverseLink/FabricLink_SQLAnalyticsEndpoint/DVFabricLinkUtil/\"\n","\n","DERIVED_TABLE_MAP_PATH = \"./builtin/resources/derived_table_map.json\"\n","LH_DDL_TEMPLATE_PATH = \"./builtin/resources/get_lh_ddl_as_view.sql\"\n","VIEW_DEPENDENCY_TEMPLATE_PATH = \"./builtin/resources/get_view_dependency.sql\"\n","\n","# --- Required resource files ---\n","REQUIRED_FILES = [\n","    (\"derived_table_map.json\", DERIVED_TABLE_MAP_PATH),\n","    (\"get_lh_ddl_as_view.sql\", LH_DDL_TEMPLATE_PATH),\n","    (\"get_view_dependency.sql\", VIEW_DEPENDENCY_TEMPLATE_PATH)]\n","   \n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"87c8eac9-bb26-4c3c-88fb-9a18853c1674","normalized_state":"finished","queued_time":"2025-04-06T20:09:15.8309736Z","session_start_time":null,"execution_start_time":"2025-04-06T20:09:15.832003Z","execution_finish_time":"2025-04-06T20:09:16.2170644Z","parent_msg_id":"e9ffe9c6-46a5-4b59-a1fe-64a95bc57e70"}},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"50733e90-a4fc-4424-896b-16399342f769"},{"cell_type":"code","source":["# --- Imports ---\n","import requests\n","import json\n","import logging\n","import time\n","import struct\n","import urllib.parse\n","import pandas as pd\n","from IPython.display import display, JSON\n","from sqlalchemy import create_engine, text, event\n","from requests.exceptions import HTTPError\n","import sys\n","import os\n","\n","\n","def download_file_if_not_exists(url, local_path):\n","    \"\"\"Download a file from GitHub if it doesn't exist locally.\"\"\"\n","    if not notebookutils.fs.exists(local_path):\n","        notebookutils.fs.mkdirs(os.path.dirname(local_path))\n","        logger.info(f\"‚¨áÔ∏è Downloading {local_path} ...\")\n","        response = requests.get(url)\n","        response.raise_for_status()  # Fail if not 200 OK\n","        notebookutils.fs.put(local_path, response.content.decode('utf-8'))  # <-- decode bytes to string\n","    else:\n","        logger.info(f\"üìÑ File already exists locally: {local_path}\")\n","\n","\n","# --- Database Connections ---\n","def create_synapse_engine(server, database):\n","    connection_string = f\"DRIVER={DRIVER};SERVER={server};DATABASE={database};Encrypt=yes;TrustServerCertificate=no;\"\n","    odbc_conn_str = f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(connection_string)}\"\n","    engine = create_engine(odbc_conn_str)\n","    return engine\n","\n","synapse_edl_engine = create_synapse_engine(SYNAPSE_SERVER, SYNAPSE_EDL_DATABASE)\n","synapse_dw_engine = create_synapse_engine(SYNAPSE_SERVER, SYNAPSE_DW_DATABASE)\n","\n","@event.listens_for(synapse_edl_engine, \"do_connect\")\n","@event.listens_for(synapse_dw_engine, \"do_connect\")\n","def inject_access_token(dialect, conn_rec, cargs, cparams):\n","    token = notebookutils.credentials.getToken(\"https://database.windows.net/\")\n","    token_bytes = token.encode(\"utf-16-le\")\n","    token_struct = struct.pack(f\"<I{len(token_bytes)}s\", len(token_bytes), token_bytes)\n","    cparams[\"attrs_before\"] = {1256: token_struct}\n","\n","# --- Utility Functions ---\n","def load_builtin_file(path):\n","    \"\"\"Load a built-in SQL or JSON file.\"\"\"\n","    return notebookutils.fs.head(path)\n","\n","def connect_to_fabric_artifact(artifact_name, workspace_id):\n","    return notebookutils.data.connect_to_artifact(artifact_name, workspace_id)\n","\n","# --- Warehouse Management ---\n","def warehouse_exists(workspace_id, warehouse_name):\n","    try:\n","        connect_to_fabric_artifact(warehouse_name, workspace_id)\n","        return True\n","    except Exception as e:\n","        if \"ArtifactNotFoundException\" in str(type(e)):\n","            logger.info(f\"üîç Warehouse not found:{warehouse_name}\")\n","            return False\n","        raise\n","\n","def create_case_insensitive_warehouse(workspace_id, warehouse_name, retries=5, delay=10):\n","    if warehouse_exists(workspace_id, warehouse_name):\n","        logger.info(f\"‚úÖ Warehouse '{warehouse_name}' already exists.\")\n","        return True\n","\n","    logger.info(f\"üèóÔ∏è Creating new warehouse:{warehouse_name}\")\n","    token = notebookutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")\n","    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n","    payload = {\n","        \"type\": \"Warehouse\",\n","        \"displayName\": warehouse_name,\n","        \"description\": \"New warehouse with case-insensitive collation\",\n","        \"creationPayload\": {\"defaultCollation\": \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"}\n","    }\n","\n","    response = requests.post(\n","        f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\",\n","        headers=headers, data=json.dumps(payload)\n","    )\n","    logger.info(f\"üì® Warehouse creation response: {response.text}\", )\n","\n","    for attempt in range(retries):\n","        time.sleep(delay)\n","        if warehouse_exists(workspace_id, warehouse_name):\n","            logger.info(\"‚úÖ Warehouse is ready.\")\n","            return True\n","        logger.info(f\"üîÑ Retry {attempt + 1}/{retries}: Warehouse not yet available.\")\n","\n","    logger.error(\"‚ùå Failed to create warehouse after retries.\")\n","    return False\n","\n","# --- Schema Retrieval ---\n","def fetch_tables_and_schema_map(engine):\n","    \"\"\"Fetch table list and schema mapping.\"\"\"\n","    table_list_query = f\"\"\"\n","        SELECT STRING_AGG(CONVERT(NVARCHAR(MAX), TABLE_NAME), ',') AS tablelist\n","        FROM (\n","            SELECT DISTINCT LOWER(TABLE_NAME) as TABLE_NAME\n","            FROM INFORMATION_SCHEMA.COLUMNS\n","            WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' \n","              AND TABLE_NAME IN (\n","                  SELECT DISTINCT TABLE_NAME \n","                  FROM INFORMATION_SCHEMA.COLUMNS\n","                  WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' \n","                    AND COLUMN_NAME = '{SYNAPSE_EDL_CONST_COLUMN_NAME}'\n","              )\n","        ) AS tbl\n","    \"\"\"\n","    schema_map_query = \"\"\"\n","        SELECT TABLE_NAME, COLUMN_NAME,\n","            DATA_TYPE + \n","            CASE \n","                WHEN CHARACTER_MAXIMUM_LENGTH IS NOT NULL THEN '(' + CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)) + ')'\n","                WHEN CHARACTER_MAXIMUM_LENGTH = -1 THEN '(max)'\n","                WHEN DATA_TYPE = 'decimal' THEN '(' + CAST(NUMERIC_PRECISION AS VARCHAR(10)) + ',' + CAST(NUMERIC_SCALE AS VARCHAR(10)) + ')'\n","                ELSE '' \n","            END AS datatype\n","        FROM INFORMATION_SCHEMA.COLUMNS\n","        WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}'\n","          AND TABLE_NAME IN (\n","              SELECT DISTINCT TABLE_NAME \n","              FROM INFORMATION_SCHEMA.COLUMNS\n","              WHERE TABLE_SCHEMA = '{SYNAPSE_EDL_SCHEMA}' AND COLUMN_NAME = '{SYNAPSE_EDL_CONST_COLUMN_NAME}'\n","          )\n","    \"\"\"\n","\n","    with engine.connect() as conn:\n","        table_list = conn.execute(text(table_list_query)).scalar()\n","    schema_map_df = pd.read_sql(schema_map_query, engine)\n","    schema_map_json = schema_map_df.to_json(orient=\"records\")\n","    return table_list, schema_map_json\n","\n","# --- DDL and View Creation ---\n","def generate_lh_view_ddl(lakehouse_name, params):\n","    \"\"\"Generate view DDL based on Lakehouse tables.\"\"\"\n","    conn = connect_to_fabric_artifact(lakehouse_name, WORKSPACE_ID)\n","    variables_script = f\"\"\"\n","        DECLARE \n","            @source_database_name VARCHAR(200) = '{lakehouse_name}',\n","            @source_table_schema NVARCHAR(10) = '{params[\"source_schema\"]}',\n","            @target_table_schema NVARCHAR(10) = '{params[\"target_schema\"]}',\n","            @TablesToInclude_FnOOnly INT = {params[\"only_fno_tables\"]},\n","            @TablesToIncluce NVARCHAR(MAX) = '{params[\"tables_to_include\"]}',\n","            @TablesToExcluce NVARCHAR(MAX) = '{params[\"tables_to_exclude\"]}',\n","            @filter_deleted_rows INT = {params[\"filter_deleted_rows\"]},\n","            @join_derived_tables INT = {params[\"join_derived_tables\"]},\n","            @change_column_collation INT = {params[\"change_collation\"]},\n","            @translate_enums INT = {params[\"translate_enums\"]},\n","            @schema_map VARCHAR(MAX) = '{params[\"schema_map\"]}',\n","            @tableinheritance NVARCHAR(MAX) = LOWER('{params[\"derived_table_map\"]}');\n","    \"\"\"\n","    sql_template = load_builtin_file(LH_DDL_TEMPLATE_PATH)\n","    df = conn.query(f\"{variables_script} {sql_template}\")\n","    return df.iloc[0, 0]\n","\n","def execute_ddl_on_warehouse(warehouse_name, ddl_query):\n","    \"\"\"Execute DDL in warehouse.\"\"\"\n","    conn = connect_to_fabric_artifact(warehouse_name, WORKSPACE_ID)\n","    conn.query(ddl_query)\n","\n","# --- View Dependency Management ---\n","def fetch_view_dependencies(engine, root_entities, old_db, new_db, old_schema, new_schema):\n","    sql_template = load_builtin_file(VIEW_DEPENDENCY_TEMPLATE_PATH)\n","    sql_script = f\"\"\"\n","        SET NOCOUNT ON;\n","        DROP TABLE IF EXISTS #myEntitiestree;\n","        DECLARE @entities NVARCHAR(MAX) = '{root_entities}';\n","        DECLARE @old_schema VARCHAR(10) = '{old_schema}';\n","        DECLARE @new_schema VARCHAR(10) = '{new_schema}';\n","        {sql_template}\n","    \"\"\"\n","    with engine.connect() as conn:\n","        result = conn.execute(text(sql_script))\n","        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n","    df[\"definition\"] = df[\"definition\"].str.replace(old_db, new_db, case=False, regex=False)\n","    return df.to_json(orient=\"records\")\n","\n","def deploy_views(warehouse_name, schema, views_json):\n","    execute_ddl_on_warehouse(warehouse_name, f\"IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = '{schema}') BEGIN EXEC('CREATE SCHEMA {schema}'); END;\")\n","    views = json.loads(views_json)\n","    for view in sorted(views, key=lambda x: x[\"depth\"], reverse=True):\n","        if view.get(\"definition\"):\n","            try:\n","                entityname = view[\"entityName\"]\n","                execute_ddl_on_warehouse(warehouse_name, view[\"definition\"])\n","                logger.info(f\"‚úÖ Deployed view: {entityname}\")\n","            except Exception as e:\n","                logger.error(f\"‚ùå Failed to deploy view {entityname}: {e}\")\n","\n","# --- Main Execution ---\n","def main():\n","\n","    for handler in logging.root.handlers[:]:  # Clear existing handlers\n","        logging.root.removeHandler(handler)\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        format=\"[%(asctime)s] %(levelname)s - %(message)s\",\n","        datefmt=\"%Y-%m-%d %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)]  # Explicitly use stdout\n","    )\n","    \n","    logger.info(\"üèóÔ∏è Step 1: Download template files if does not exists.\")\n","    # --- Download if not exists ---\n","    for filename, local_path in REQUIRED_FILES:\n","        file_url = f\"{GITHUB_RAW_BASE_URL}/{filename}\"\n","        download_file_if_not_exists(file_url, local_path)\n","        \n","    logger.info(\"üèóÔ∏è Step 2/7: Ensure case-insensitive warehouse exists.\")\n","    if create_case_insensitive_warehouse(WORKSPACE_ID, FABRIC_WH_DATABASE):\n","        logger.info(\"üîπ Step 3/7: Fetch tables and schema map from Synapse.\")\n","        tables_to_include, schema_map = fetch_tables_and_schema_map(synapse_edl_engine)\n","\n","        logger.info(\"üîÅ Step 4/7: Load derived table map.\")\n","        derived_table_map = load_builtin_file(DERIVED_TABLE_MAP_PATH)\n","\n","        logger.info(\"üõ†Ô∏è Step 5/7: Generate view DDL.\")\n","        params = {\n","            \"source_schema\": \"dbo\",\n","            \"target_schema\": \"dbo\",\n","            \"only_fno_tables\": 1,\n","            \"tables_to_include\": tables_to_include,\n","            \"tables_to_exclude\": \"*\",\n","            \"filter_deleted_rows\": 1,\n","            \"join_derived_tables\": 1,\n","            \"change_collation\": 1,\n","            \"translate_enums\": 0,\n","            \"schema_map\": schema_map,\n","            \"derived_table_map\": derived_table_map\n","        }\n","        view_ddl = generate_lh_view_ddl(FABRIC_LH_DATABASE, params)\n","        execute_ddl_on_warehouse(FABRIC_WH_DATABASE, view_ddl)\n","\n","        logger.info(\"üöÄ Step 6/7: Fetch view dependencies.\")\n","        views_json = fetch_view_dependencies(\n","            synapse_dw_engine, SYNAPSE_DW_VIEWS,\n","            SYNAPSE_EDL_DATABASE, FABRIC_WH_DATABASE,\n","            SYNAPSE_DW_SCHEMA, FABRIC_WH_SCHEMA\n","        )\n","\n","        logger.info(\"üöÄ Step 7/7: Deploy views to warehouse.\")\n","        deploy_views(FABRIC_WH_DATABASE, FABRIC_WH_SCHEMA, views_json)\n","        logger.info(\"üéâ Deployment complete.\")\n","    else:\n","        logger.error(\"‚ùå Warehouse setup failed.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"87c8eac9-bb26-4c3c-88fb-9a18853c1674","normalized_state":"finished","queued_time":"2025-04-06T20:12:44.8076535Z","session_start_time":null,"execution_start_time":"2025-04-06T20:12:44.8087584Z","execution_finish_time":"2025-04-06T20:12:50.8129138Z","parent_msg_id":"6ee699ea-9d8d-4dfc-bb5c-4a46856262c0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[2025-04-06 20:12:44] INFO - üèóÔ∏è Step 1: Download template files if does not exists.\n[2025-04-06 20:12:44] INFO - üìÑ File already exists locally: ./builtin/resources/derived_table_map.json\n[2025-04-06 20:12:45] INFO - üìÑ File already exists locally: ./builtin/resources/get_lh_ddl_as_view.sql\n[2025-04-06 20:12:45] INFO - üìÑ File already exists locally: ./builtin/resources/get_view_dependency.sql\n[2025-04-06 20:12:45] INFO - üèóÔ∏è Step 2/7: Ensure case-insensitive warehouse exists.\n[2025-04-06 20:12:45] INFO - ‚úÖ Warehouse 'dataverse_analytics_warehouse1' already exists.\n[2025-04-06 20:12:45] INFO - üîπ Step 3/7: Fetch tables and schema map from Synapse.\n[2025-04-06 20:12:46] INFO - üîÅ Step 4/7: Load derived table map.\n[2025-04-06 20:12:46] INFO - üõ†Ô∏è Step 5/7: Generate view DDL.\n[2025-04-06 20:12:49] INFO - üöÄ Step 6/7: Fetch view dependencies.\n[2025-04-06 20:12:49] INFO - üöÄ Step 7/7: Deploy views to warehouse.\n[2025-04-06 20:12:49] INFO - ‚úÖ Deployed view: DirPartyPrimary_view\n[2025-04-06 20:12:49] INFO - ‚úÖ Deployed view: CustomerDim\n[2025-04-06 20:12:49] INFO - üéâ Deployment complete.\n"]}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"8c2d3a9a-f5c8-4078-a36f-b540e44e6bfd"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","language":"Jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[]},"warehouse":{"known_warehouses":[{"id":"09b67508-1825-42b8-b9f4-6946b02e8177","type":"Datawarehouse"}],"default_warehouse":"09b67508-1825-42b8-b9f4-6946b02e8177"}}},"nbformat":4,"nbformat_minor":5}